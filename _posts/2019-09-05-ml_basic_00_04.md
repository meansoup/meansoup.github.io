---
layout: post
title: deep learning basic 00-04
tag:
  - machineLearning
---

[모두를 위한 머신러닝 강의](https://hunkim.github.io/ml/)의 굉장히 체계적으로 정리된 동영상 강의와 자료를 사용하여 Machine Learning에 대해 공부한다.  
감사하게도 고퀄리티 강의가 한국어!

## Lecture 0:

### machine/deep learning
이 시대의 super power라고 볼 수 있다.  
`tenserflow`와 `python`을 사용한다.

## Lecture 1

### machine learning
스스로 학습해서 상황에 맞춰 수행하는 것으로 일종의 소프트웨어이다.  
기존의 코드들은 `explicit programming`으로, 개발자가 상황에 맞춰 만들어 놓은 로직대로 움직이는 것이다.

### supervised learning  
대부분의 ML은 supervised에 속하며, 정해진 data로 learning을 하는 것을 말한다.
  - 예를 들면, 고양이 사진을 모아서 cat이라고 label을 달아놓고 학습 시킴.
    - taged image
    - spam mail

| type                       | desc                        |
| -------------------------- | --------------------------- |
| regression                 | 0 ~ 100 등의 범위가 넓은 것         |
| binary classification      | pass/non-pass로 구분하는 것       |
| multi-label classification | A, B, C, D 등의 grade로 구분하는 것 |

### unsupervised learning
label이 없는 data로 학습시키는 것.
  - 예를 들면, 구글 뉴스와 같은 것들로 학습 시킴.

### AlphaGo
알파고도 기존에 있던 기보(training data set)를 학습한 것으로, supervised learning을 한 것이다.

## Lecture 2

### Linear Regression
가지고 있는 data set에 잘 맞는 2차원 line을 그린다고 생각하면 된다.  
그리고 받은 값에서 해당 line에 맞는 값들을 그린다고 생각하면 된다. (`y = ax + b`)  

### Hypothesis
`H(x) = Wx + b` 라는 가정에 대한 함수를 갖는다.

### cost function
![01](study/img/machine_learning/lec/02_01.png)
`loss function`이라고도 한다.
liner regression으로 그려진 line과 주어진 값과의 차이를 계산하는 것으로 작을수록 좋다. 가장 작도록 하는 값을 찾는다.

## Lecture 3


### cost function minimalize
![02](study/img/machine_learning/lec/03_01.png)
이렇게 W 값에 따른 cost를 계산할 수 있다. (위 예제에서는 간략히 `y = wx`로 사용)  
![03](study/img/machine_learning/lec/03_02.png)
위와 같은 그래프가 나오는데, `cost(W)`가 가장 작은 값이 되는 W를 구해야 한다.

### Gradient descent algorithm
경사가 있는 경우 내려가는 방식으로, 위 그래프에서 경사가 없을 때까지 내려가는 방식의 알고리즘이다.  
따라서 어느 점에서 시작하든 W, b를 바꾸면서 `cost(W, b)`를 줄여나가는 방식으로 반복한다.  
minimize cost function으로 많이 사용된다.  
`cost(W1, W2, W3 ..., b)` 같은 경우도 minimize할 수 있는 알고리즘으로 많은 machine learning에서 사용된다. 

![04](study/img/machine_learning/lec/03_03.png)
위의 수식을 정리하면(강의자료 참고) 최종적으로 나오는 수식으로, 이를 반복하여 최적의 값을 구할 수 있다.

### convex function
![05](study/img/machine_learning/lec/03_04.png)
시작 점에 따라 위와 같이 찾게되는 값이 다르게 될 수 있다. 
![06](study/img/machine_learning/lec/03_05.png)
그치만 대부분은 위와 같은 모양을 갖게 된다.  
위와 같이 밥그릇을 엎어놓은 모양을 **convex function**이라고 하는데, 어디서 시작하든 하나의 값으로 가게된다. 따라서 항상 답을 찾는다는 것을 보장해준다.  
그래서 중요한 점은 `cost(W, b)`(cost function)을 설계할 때 convex function이 되도록하는 것이다.  

## Lecture 4


### Hypothesis for multi value
결과를 가정하기 위한 값들이 여러 개일 수 있다.  
예를 들면 세 번의 퀴즈의 값을 통해 시험 점수를 예측하는 경우는, 3개의 값으로 결과를 예측한다.  
여러 값에 대한 가정은 아래와 같은 Hypothesis를 갖는다고 볼 수 있다.  
`H(x1, x2, x3, ...) = w1x1 + w2x2 + w3x3 + ... + b`

### Matrix
![07](study/img/machine_learning/lec/04_01.png)
n개의 많은 값을 사용하게 된다면, 위와 같은 Hypothesis를 사용하기 어렵고, matrix를 사용한다.  
![08](study/img/machine_learning/lec/04_02.png)
보통 위와 같은 matrix를 갖게 된다.  
[5, 3] 에서 5는 몇 개의 instance를 가지고 있느냐이며, 3은 연산에 사용되는 variable의 개수를 말한다. [5, 1] 에서 5는 역시 instacne의 개수이고, 1은 linear regression 이므로 하나의 값을 갖게 될 것이다.
[?, ?] 이 따라서 [3, 1]임을 알 수 있다.  
matrix 곱셈을 떠올리면 좋을 것 같다.  

